# PingCAPinterview

the interview for PingCAP


## 时间 
开始时间：北京时间 2018年12月17日，下午2:00
结束时间：北京时间 2018年12月24日，下午2:00

## 作业要求
在GitHub上实现，截止时间前提交项目链接。

## 题目
100GB url文件，使用1GB内存计算出出现次数top100的url和出现的次数。

## 提示
- 注意代码可读性，添加必要的注释(英文)
- 注意代码风格与规范，添加必要的单元测试和文档。
- 注意异常处理，尝试优化性能

-----

# 思路及相关实现

## 实现思路
利用hash，将所有字符串平均分配到n个文件中，在这里n取500。  
这样即可保证所有字符串一定都在同一个文件中，我们可以考虑使用`hashmap<string,int>`，去存储每一个文件中的情况。  
这样所需要的运行时内存会非常小(某种意义上可以看成，大量的string压缩成了一个int表示)。  
最后，维护一个大小为100的小根堆，去对所有的`hashmap<string,int>`进行遍历，最终堆中存在的，就是100个出现次数最频繁的url。  

## 遇到的一些问题
1. url可能有多个，如:
	- https://baidu.com
	- http://baidu.com
	- www.baidu.com
	- baidu.com/
	- www.baidu.com/
	- ……
   在这多个的情况中，考虑将所有url在读入的时候均处理为baidu.com/，以此来避免多个写法成了一个。
2. 对于运行时内存的检测和测试
	- 使用/usr/bin/time -v命令，来获取详细信息，写脚本进行统计。
3. 对于正确性的检验。
	- 产生较小的数据，利用暴力程序进行检测。(暴力程序不能直接输出，因为堆的维护关系，导致暴力程序输出顺序会不太一样，这里解决方案是进行一些判断)

## 进一步优化思路
1. 使用或参考hadoop、map_reduce、pandas等相关实现，进一步优化。
2. 利用分布式、多线程等方式，对于hash出来的多个文件同步进行处理。(对于hash本身，因计算不大，不一定需要多线程)

## TODO
1. 调整或证明hash函数，严格保证内存不会超限制。
2. 进一步调整文件的使用，减少硬盘占用情况。
3. 进一步完善测试，对于测试更加严格，进一步约束和判断。
4. 改善代码风格。
